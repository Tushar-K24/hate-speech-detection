{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_l-5EkZWMvN7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10896\\2474201762.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "class CFG:\n",
    "    class data:\n",
    "        batch_size=32\n",
    "        validation_size = 0.2\n",
    "        lr = 1e-4\n",
    "        epochs = 10  \n",
    "        epsilon = 1e-8\n",
    "        MAX_LEN = 128 #max sentence length\n",
    "        seed_val = 42 #random seed\n",
    "        k_folds = 10\n",
    "        hidden_size = 768 #hidden layer size (embedding size) for feedforward net\n",
    "#         PATH = \"/content/drive/MyDrive/Minor Project1/\"\n",
    "        PATH = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TowdNwYDcIGT",
    "outputId": "65769906-d5bb-444d-eca8-7b625efba27a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0jQ-gIucOVS"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNZGciuecbbL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Minor Project/ETHOS_binary.csv')\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "sentences = df.text.values\n",
    "labels = df.label.values\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50McPKPccur4"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "#Load the Bert model.\n",
    "print('Loading BERT Model...')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "filepath = CFG.data.PATH + 'original.pth'\n",
    "if not os.path.exists(filepath):\n",
    "    torch.save(model.state_dict(), filepath) \n",
    "    \n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ghrz9Ca-dgki"
   },
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "                        # This function also supports truncation and conversion\n",
    "                        # to pytorch tensors, but we need to do padding, so we\n",
    "                        # can't use these features :( .\n",
    "                        #max_length = 128,          # Truncate all sentences.\n",
    "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9JJKVVUeD7W"
   },
   "outputs": [],
   "source": [
    "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syDi788LeLN2"
   },
   "outputs": [],
   "source": [
    "# We'll borrow the `pad_sequences` utility function to do this.\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % CFG.data.MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "# Pad our input tokens with value 0.\n",
    "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
    "# as opposed to the beginning.\n",
    "input_ids = pad_sequences(input_ids, maxlen=CFG.data.MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMIV6zd7eQ0N"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyXWDkCS0bjn"
   },
   "outputs": [],
   "source": [
    "#one-hot encode labels\n",
    "def one_hot(categorical_labels):\n",
    "    num_categories = 2\n",
    "    labels = np.zeros((categorical_labels.shape[0], num_categories))\n",
    "    labels[:,0] = (categorical_labels == 0).astype(int)\n",
    "    labels[:,1] = (categorical_labels == 1).astype(int)\n",
    "    return labels\n",
    "\n",
    "labels = one_hot(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FuiF82pefHL"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model.\n",
    "# train_inputs = torch.tensor(train_inputs)\n",
    "# validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "# train_labels = torch.tensor(train_labels)\n",
    "# validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "# train_masks = torch.tensor(train_masks)\n",
    "# validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "labels = torch.tensor(labels)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "dataset = TensorDataset(input_ids, labels, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YryQaWoHeg3O"
   },
   "outputs": [],
   "source": [
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = CFG.data.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKrVeeODfAKd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "# def flat_accuracy(preds, labels):\n",
    "#     labels_flat = labels.flatten()\n",
    "#     return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    y_hat = np.argmax(preds, axis=1).flatten()\n",
    "    y = labels[:,1].flatten()\n",
    "      \n",
    "    #find tp, tn, fp, fn\n",
    "    tp = ((y==1) & (y_hat==1)).sum()\n",
    "    tn = ((y==0) & (y_hat==0)).sum()\n",
    "    fp = ((y==0) & (y_hat==1)).sum()\n",
    "    fn = ((y==1) & (y_hat==0)).sum()\n",
    "    total = len(labels)\n",
    "    return tp, tn, fp, fn, total\n",
    "    \n",
    "def get_accuracy(tp, tn, fp, fn, total):\n",
    "    epsilon = CFG.data.epsilon\n",
    "\n",
    "    def accuracy():\n",
    "      return (tp + tn)/(total + epsilon) \n",
    "\n",
    "    def precision():\n",
    "      return tp/(tp + fp + epsilon)\n",
    "    \n",
    "    def recall():\n",
    "      return tp/(tp + fn + epsilon)\n",
    "\n",
    "    def f1():\n",
    "      p = precision()\n",
    "      r = recall()\n",
    "      return 2*p*r/(p + r + epsilon)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy(),\n",
    "        'precision': precision(),\n",
    "        'recall': recall(),\n",
    "        'f1-score': f1()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6KMjqE6fSiW"
   },
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "for layer in range(12):\n",
    "    model.bert.encoder.layer[layer].output.LayerNorm.register_forward_hook(get_activation(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kabezlmIRZR4"
   },
   "outputs": [],
   "source": [
    "input_ids_ = input_ids[:5].to(device)\n",
    "input_mask_ = attention_masks[:5].to(device)\n",
    "input_labels_ = labels[:5].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fY2MArWrb1B"
   },
   "outputs": [],
   "source": [
    "input_ids_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1p-LMYbdJQNb"
   },
   "outputs": [],
   "source": [
    "outputs = model(input_ids_, \n",
    "                token_type_ids=None, \n",
    "                attention_mask=input_mask_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TtYbpiAqsjK"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "a1 = copy.deepcopy(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Is3VVPn0q7Bv"
   },
   "outputs": [],
   "source": [
    "print(a1[11].shape)\n",
    "a1[11].mean(dim=1).shape\n",
    "a1[11][:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doEh0c6dp4vs"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(CFG.data.hidden_size, CFG.data.hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p = 0.2)\n",
    "        self.fc1 = nn.Linear(CFG.data.hidden_size, 2) #input_size, output_size\n",
    "        # self.fc2 = nn.Linear(64,64)\n",
    "        # self.fc3 = nn.Linear(64,64)\n",
    "        # self.fc4 = nn.Linear(64,10) #10 because 10 classes {0,1,2,3,4,5,6,7,8,9}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        # x = self.tanh(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # x = F.relu(self.fc3(x))\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVfZjNNIpMq3"
   },
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "for layer in range(12):\n",
    "    model.bert.encoder.layer[layer].output.LayerNorm.register_forward_hook(get_activation(layer))\n",
    "\n",
    "def prepare_data(inputs, masks, labels):\n",
    "    inputs = inputs.to(device)\n",
    "    masks = masks.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(inputs, \n",
    "                token_type_ids=None, \n",
    "                attention_mask=masks)\n",
    "    # inputs = activation[11][:,0,:]\n",
    "#     inputs = activation[11].mean(dim=1) #exp1: taking mean of all tokens in last layer\n",
    "    \n",
    "    #exp2\n",
    "    cnt = 1e-8\n",
    "    inputs = torch.zeros_like(activation[0][:,0,:])\n",
    "    for i in range(9,12):\n",
    "      cnt += 1\n",
    "      inputs += activation[i][:,0,:]\n",
    "    inputs/=cnt\n",
    "    \n",
    "    return [inputs, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2ypIVvfs4j5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "606CoOFssfWD"
   },
   "outputs": [],
   "source": [
    "batch_size = CFG.data.batch_size\n",
    "\n",
    "def train1(train_data, validation_data, fold_id, exp_no):\n",
    "    '''\n",
    "      Data: (list) -> [inputs, masks, labels]\n",
    "    '''\n",
    "    \n",
    "    net = Net()\n",
    "    net.cuda()\n",
    "\n",
    "    # Create the DataLoader for our training set.\n",
    "    train_data = TensorDataset(*train_data)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create the DataLoader for our validation set.\n",
    "    validation_data = TensorDataset(*validation_data)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Function to validate model\n",
    "    def validate_model(validation_dataloader):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        net.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        tp, tn, fp, fn, total = 0, 0, 0, 0, 0\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            \n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "            # Telling the model not to compute or store gradients, saving memory and\n",
    "            # speeding up validation\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # This will return the logits rather than the loss because we have\n",
    "                # not provided labels.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                b_inputs, b_labels = prepare_data(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                logits = net(b_inputs)\n",
    "\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            \n",
    "            # Calculate the accuracy for this batch of test sentences.\n",
    "            tp_, tn_, fp_, fn_, tot_ = flat_accuracy(logits, label_ids)\n",
    "            \n",
    "            tp+=tp_\n",
    "            tn+=tn_\n",
    "            fp+=fp_\n",
    "            fn+=fn_\n",
    "            total+=tot_\n",
    "\n",
    "            # Track the number of batches\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        acc_metrics = get_accuracy(tp, tn, fp, fn, total)\n",
    "        for k, v in acc_metrics.items():\n",
    "          print(\"{} : {:.5f},\".format(k,v), end=\" \")\n",
    "\n",
    "        print(\"\\nValidation took: {:}\".format(format_time(time.time() - t0)))\n",
    "        return acc_metrics\n",
    "    \n",
    "\n",
    "    best_metrics = {}\n",
    "    best_f1_so_far = 0.0\n",
    "    \n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    optimizer = AdamW(net.parameters(),\n",
    "                      lr = CFG.data.lr, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                      eps = CFG.data.epsilon\n",
    "                    )\n",
    "    \n",
    "    #define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Total number of training steps is number of batches * number of epochs.\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "    \n",
    "    print('Current Best f1-score: {:.5}'.format(best_f1_so_far))\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        \n",
    "        # For each batch of training data...\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 1\n",
    "\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        net.train()\n",
    "\n",
    "        print('Training...')\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            b_inputs, b_labels = prepare_data(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because \n",
    "            # accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            net.zero_grad()        \n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # This will return the loss (rather than the model output) because we\n",
    "            # have provided the `labels`.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = net(b_inputs)\n",
    "            \n",
    "            # The call to `model` always returns a tuple, so we need to pull the \n",
    "            # loss value out of the tuple.\n",
    "            loss = criterion(outputs, b_labels)\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)            \n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        accuracy_metrics = validate_model(validation_dataloader)\n",
    "        current_f1 = accuracy_metrics['f1-score']\n",
    "        if current_f1>best_f1_so_far:\n",
    "            print('f1 score improved, old f1 = {:.5}, new f1 = {:.5}'.format(best_f1_so_far, current_f1))\n",
    "            best_f1_so_far = current_f1\n",
    "            best_metrics = accuracy_metrics\n",
    "            print('Saving new weights...')\n",
    "            torch.save(net.state_dict(), filepath + f'net_exp{exp_no}_{fold_id}.pth')\n",
    "\n",
    "        elif current_f1 == best_f1_so_far:\n",
    "            print('f1 score did not change')\n",
    "            b_acc = best_metrics['accuracy']\n",
    "            cur_acc = accuracy_metrics['accuracy']\n",
    "            if cur_acc>b_acc:\n",
    "                print('accuracy score improved, old f1 = {:.5}, new f1 = {:.5}'.format(b_acc, cur_acc))\n",
    "                best_metrics = accuracy_metrics\n",
    "                print('Saving new weights...')\n",
    "                torch.save(net.state_dict(), filepath + f'net_exp{exp_no}_{fold_id}.pth')\n",
    "\n",
    "    return best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZme482GnF3A"
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=CFG.data.k_folds, shuffle=True)\n",
    "best_f1_scores = {}\n",
    "model.train(False)\n",
    "\n",
    "for fold_id, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "\n",
    "    print('------------------ Fold {} ------------------'.format(fold_id))\n",
    "\n",
    "    train_inputs, train_masks, train_labels = input_ids[train_ids], attention_masks[train_ids], labels[train_ids]\n",
    "    validation_inputs, validation_masks, validation_labels = input_ids[val_ids], attention_masks[val_ids], labels[val_ids]\n",
    "    train_data = [train_inputs, train_masks, train_labels]\n",
    "    validation_data = [validation_inputs, validation_masks, validation_labels]\n",
    "\n",
    "    best_f1_scores[fold_id] = train1(train_data, validation_data, fold_id, 1)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkS6jdEKsVnf"
   },
   "outputs": [],
   "source": [
    "for fold_id in range(10):\n",
    "  print(f\"{fold_id}: \", end = \" \")\n",
    "  for k,v in best_f1_scores[fold_id]:\n",
    "    print(f\"{k}: {:.5v}\", end=\", \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "becU_3E88q-2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
